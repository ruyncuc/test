{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert4keras_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPSgUwtDqGa8OUGbwqXjR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruyncuc/test/blob/master/bert4keras_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGmdj_sqUpiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "295803c0-0b73-43d6-bf2a-cf9c765c7b56"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qrfLJ7iVOOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f232acd7-512b-4519-a0bb-e73f7651a66b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-tIg2lVhZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "250200ef-7872-4ef1-f7ed-570cc8d77fb7"
      },
      "source": [
        "! wget http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
        "! tar -zxvf china-people-daily-ner-corpus.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-18 08:11:43--  http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
            "Resolving s3.bmio.net (s3.bmio.net)... 52.219.0.67\n",
            "Connecting to s3.bmio.net (s3.bmio.net)|52.219.0.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2443473 (2.3M) [application/x-gzip]\n",
            "Saving to: ‘china-people-daily-ner-corpus.tar.gz’\n",
            "\n",
            "china-people-daily- 100%[===================>]   2.33M  3.67MB/s    in 0.6s    \n",
            "\n",
            "2020-04-18 08:11:44 (3.67 MB/s) - ‘china-people-daily-ner-corpus.tar.gz’ saved [2443473/2443473]\n",
            "\n",
            "./._china-people-daily-ner-corpus\n",
            "china-people-daily-ner-corpus/\n",
            "china-people-daily-ner-corpus/._example.dev\n",
            "china-people-daily-ner-corpus/example.dev\n",
            "china-people-daily-ner-corpus/._example.train\n",
            "china-people-daily-ner-corpus/example.train\n",
            "china-people-daily-ner-corpus/._example.test\n",
            "china-people-daily-ner-corpus/example.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-JZyMeLV3dT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "4626626d-0d2a-4cc8-d9e8-d9395df7628d"
      },
      "source": [
        "pip install git+https://www.github.com/bojone/bert4keras.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/bojone/bert4keras.git\n",
            "  Cloning https://www.github.com/bojone/bert4keras.git to /tmp/pip-req-build-m4vjypn8\n",
            "  Running command git clone -q https://www.github.com/bojone/bert4keras.git /tmp/pip-req-build-m4vjypn8\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras==0.7.3) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (1.18.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.7.3) (3.13)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.7.3-cp36-none-any.whl size=39430 sha256=4b09136f37142889982cf533eb20e2cfec56aa51fe6d37a635d0637d2f7eeb89\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w_e8fbf2/wheels/12/58/83/8ff5c864b80c860e6d9e9e0d90c04fafca05d01d21f9f6fcba\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuUsITqfWRV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5afe639-ee65-4298-8ede-f48af21798c4"
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "# 用CRF做中文命名实体识别\n",
        "# 数据集 http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
        "# 实测验证集的F1可以到96.18%，测试集的F1可以到95.35%\n",
        "\n",
        "import numpy as np\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from bert4keras.layers import ConditionalRandomField\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "maxlen = 256\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "bert_layers = 12\n",
        "learing_rate = 1e-5  # bert_layers越小，学习率应该要越大\n",
        "crf_lr_multiplier = 1000  # 必要时扩大CRF层的学习率\n",
        "\n",
        "config_path = '/content/drive/My Drive/chinese_L-12_H-768_A-12/bert_config.json'\n",
        "checkpoint_path = '/content/drive/My Drive/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
        "dict_path = '/content/drive/My Drive/chinese_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    D = []\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        f = f.read()\n",
        "        for l in f.split('\\n\\n'):\n",
        "            if not l:\n",
        "                continue\n",
        "            d, last_flag = [], ''\n",
        "            for c in l.split('\\n'):\n",
        "                char, this_flag = c.split(' ')\n",
        "                if this_flag == 'O' and last_flag == 'O':\n",
        "                    d[-1][0] += char\n",
        "                elif this_flag == 'O' and last_flag != 'O':\n",
        "                    d.append([char, 'O'])\n",
        "                elif this_flag[:1] == 'B':\n",
        "                    d.append([char, this_flag[2:]])\n",
        "                else:\n",
        "                    d[-1][0] += char\n",
        "                last_flag = this_flag\n",
        "            D.append(d)\n",
        "    return D\n",
        "\n",
        "\n",
        "# 标注数据\n",
        "# 载入数据，注意已经把下载好的人民日报数据放到了./sample_data文件夹内\n",
        "train_data = load_data('/content/china-people-daily-ner-corpus/example.train')\n",
        "valid_data = load_data('/content/china-people-daily-ner-corpus/example.dev')\n",
        "test_data = load_data('/content/china-people-daily-ner-corpus/example.test')\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
        "\n",
        "# 类别映射\n",
        "labels = ['PER', 'LOC', 'ORG']\n",
        "id2label = dict(enumerate(labels))\n",
        "label2id = {j: i for i, j in id2label.items()}\n",
        "num_labels = len(labels) * 2 + 1\n",
        "\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, item in self.sample(random):\n",
        "            token_ids, labels = [tokenizer._token_start_id], [0]\n",
        "            for w, l in item:\n",
        "                w_token_ids = tokenizer.encode(w)[0][1:-1]\n",
        "                if len(token_ids) + len(w_token_ids) < maxlen:\n",
        "                    token_ids += w_token_ids\n",
        "                    if l == 'O':\n",
        "                        labels += [0] * len(w_token_ids)\n",
        "                    else:\n",
        "                        B = label2id[l] * 2 + 1\n",
        "                        I = label2id[l] * 2 + 2\n",
        "                        labels += ([B] + [I] * (len(w_token_ids) - 1))\n",
        "                else:\n",
        "                    break\n",
        "            token_ids += [tokenizer._token_end_id]\n",
        "            labels += [0]\n",
        "            segment_ids = [0] * len(token_ids)\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append(labels)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = sequence_padding(batch_labels)\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwFP3-XeXLAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0b759cc-a7ac-4ef3-bc60-978537cac3a0"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['海钓比赛地点在', 'O'], ['厦门', 'LOC'], ['与', 'O'], ['金门', 'LOC'], ['之间的海域。', 'O']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFD-YuwnXkkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "27a07d67-17ca-4cc6-f4aa-aaf632ca209d"
      },
      "source": [
        "test_data[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['我们变而以书会友，以书结缘，把', 'O'],\n",
              " ['欧', 'LOC'],\n",
              " ['美', 'LOC'],\n",
              " ['、', 'O'],\n",
              " ['港', 'LOC'],\n",
              " ['台', 'LOC'],\n",
              " ['流行的食品类图谱、画册、工具书汇集一堂。', 'O']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7jqJSVLW_1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "后面的代码使用的是bert类型的模型，如果你用的是albert，那么前几行请改为：\n",
        "model = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    model='albert',\n",
        ")\n",
        "output_layer = 'Transformer-FeedForward-Norm'\n",
        "output = model.get_layer(output_layer).get_output_at(bert_layers - 1)\n",
        "\"\"\"\n",
        "\n",
        "model = build_transformer_model(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        ")\n",
        "\n",
        "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
        "output = model.get_layer(output_layer).output\n",
        "output = Dense(num_labels)(output)\n",
        "CRF = ConditionalRandomField(lr_multiplier=crf_lr_multiplier)\n",
        "output = CRF(output)\n",
        "\n",
        "model = Model(model.input, output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss=CRF.sparse_loss,\n",
        "    optimizer=Adam(learing_rate),\n",
        "    metrics=[CRF.sparse_accuracy]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMBatVRoXFbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viterbi_decode(nodes, trans):\n",
        "    \"\"\"Viterbi算法求最优路径\n",
        "    其中nodes.shape=[seq_len, num_labels],\n",
        "        trans.shape=[num_labels, num_labels].\n",
        "    \"\"\"\n",
        "    labels = np.arange(num_labels).reshape((1, -1))\n",
        "    scores = nodes[0].reshape((-1, 1))\n",
        "    scores[1:] -= np.inf  # 第一个标签必然是0\n",
        "    paths = labels\n",
        "    for l in range(1, len(nodes)):\n",
        "        M = scores + trans + nodes[l].reshape((1, -1))\n",
        "        idxs = M.argmax(0)\n",
        "        scores = M.max(0).reshape((-1, 1))\n",
        "        paths = np.concatenate([paths[:, idxs], labels], 0)\n",
        "    return paths[:, scores[:, 0].argmax()]\n",
        "\n",
        "\n",
        "def named_entity_recognize(text):\n",
        "    \"\"\"命名实体识别函数\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    while len(tokens) > 512:\n",
        "        tokens.pop(-2)\n",
        "    mapping = tokenizer.rematch(text, tokens)\n",
        "    token_ids = tokenizer.tokens_to_ids(tokens)\n",
        "    segment_ids = [0] * len(token_ids)\n",
        "    nodes = model.predict([[token_ids], [segment_ids]])[0]\n",
        "    trans = K.eval(CRF.trans)\n",
        "    labels = viterbi_decode(nodes, trans)\n",
        "    entities, starting = [], False\n",
        "    for i, label in enumerate(labels):\n",
        "        if label > 0:\n",
        "            if label % 2 == 1:\n",
        "                starting = True\n",
        "                entities.append([[i], id2label[(label - 1) // 2]])\n",
        "            elif starting:\n",
        "                entities[-1][0].append(i)\n",
        "            else:\n",
        "                starting = False\n",
        "        else:\n",
        "            starting = False\n",
        "\n",
        "    return [\n",
        "        (text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l) for w, l in entities\n",
        "    ]\n",
        "\n",
        "\n",
        "def evaluate(data):\n",
        "    \"\"\"评测函数\n",
        "    \"\"\"\n",
        "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
        "    for d in tqdm(data):\n",
        "        text = ''.join([i[0] for i in d])\n",
        "        R = set(named_entity_recognize(text))\n",
        "        T = set([tuple(i) for i in d if i[1] != 'O'])\n",
        "        X += len(R & T)\n",
        "        Y += len(R)\n",
        "        Z += len(T)\n",
        "    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
        "    return f1, precision, recall\n",
        "\n",
        "\n",
        "class Evaluate(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.best_val_f1 = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        trans = K.eval(CRF.trans)\n",
        "        print(trans)\n",
        "        f1, precision, recall = evaluate(valid_data)\n",
        "        # 保存最优\n",
        "        if f1 >= self.best_val_f1:\n",
        "            self.best_val_f1 = f1\n",
        "            model.save_weights('./best_model.weights')\n",
        "        print(\n",
        "            'valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
        "            (f1, precision, recall, self.best_val_f1)\n",
        "        )\n",
        "        f1, precision, recall = evaluate(test_data)\n",
        "        print(\n",
        "            'test:  f1: %.5f, precision: %.5f, recall: %.5f\\n' %\n",
        "            (f1, precision, recall)\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBgdvjBWvTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluate()\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "train_generator = data_generator(train_data, batch_size)\n",
        "# 进行训练\n",
        "history = model.fit_generator(train_generator.forfit(),\n",
        "                steps_per_epoch=len(train_generator),\n",
        "                epochs=epochs,\n",
        "                callbacks=[evaluator])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKR_YPHzWyK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 预测\n",
        "text = 'MALLET是美国麻省大学（UMASS）阿姆斯特（Amherst）分校开发的一个统计自然语言处理开源软件包'\n",
        "named_entity_recognize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}